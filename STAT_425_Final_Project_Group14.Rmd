---
title: "STAT 425:Final Project"
author: "Shubham Mehta, Sharvi Tomar and Brianna Suits (Group 14)"
Group Number: "Group 14"
date: "12/12/2021"
output: pdf_document
---
# Exploratory Data Analysis
```{r Data Loading and preparation}
#loading the package
#install.packages('readxl')
library(readxl)

# Loading the data
real_estate_df = read_excel("Real_estate_valuation_data_set.xlsx",col_names = TRUE)
real_estate_df = data.frame(real_estate_df)

# Dropping index column
real_estate_df = real_estate_df[,-1]

# Renaming columns
colnames(real_estate_df) = c("txn_date","age","distance","stores","latitude","longitude","unit_area_price")

# Checking data structure
str(real_estate_df)

# Checking data dimensions
dim(real_estate_df)

# Checking for missing values
miss = sum(is.na(real_estate_df))
sprintf('Total number of missing values in the data:%i',round(miss,0))

# Creating a column for transaction month with values 1-12 
real_estate_df$txn_month = round((real_estate_df$txn_date - floor(real_estate_df$txn_date))*12,0)

#Making correction for december month
real_estate_df$txn_month[real_estate_df$txn_month == 0] = 12

#Converting date, month and number of stores to categorical
real_estate_df$txn_date = as.factor(real_estate_df$txn_date)
real_estate_df$txn_month = as.factor(real_estate_df$txn_month)
real_estate_df$stores = as.factor(real_estate_df$stores)

##Changing the levels for the month column
levels(real_estate_df$txn_month) = list(
    Jan = "1", Feb = "2", Mar = "3", Apr = "4", May = "5", Jun = "6", Jul = "7", Aug = "8", Sep = "9", Oct = "10", Nov = "11",
    Dec = "12")

# Data summary
summary(real_estate_df,digits = 10)
```

**The data is from Aug 2012 to July 2013 with transactions in every month. Thus, duration of transactions is 1 year**
**The age of purchased houses is from 0 to 43.8 years, with mean age of 17.7 years**
**The number of convenient stores range from 0 to 10, with discrete integer values**
**Average unit area price of the house is 37.9 thousands New Taiwan Dollar/Ping**

## Univariate Analysis
```{r Univariate Analysis of Month and Stores,fig.height=5, fig.width=10}
#Univariate analysis of month and stores

require(gridExtra)
library(ggplot2)

plot1 = ggplot(real_estate_df, aes(x = txn_month)) + geom_bar(fill = "steelblue")+
  ylab("Number of Transactions")+
  xlab("Month")+
  ggtitle("Month-wise transactions")+
  theme(plot.title = element_text(hjust = 0.5))

plot2 = ggplot(real_estate_df, aes(x = stores)) + geom_bar(fill = "steelblue")+
  ylab("Number of Transactions")+
  xlab("Number of Stores")+
  ggtitle("Number of transactions vs. Stores")+
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(plot1, plot2, ncol=2)

```
**The number of transactions across every month vary, with highest in May - 2013 and lowest in Jul - 2013**
**Most houses have either 0 or 5 convenience stores in the living circle**
**There are few houses (10) which have 10 stores in the living circle**

```{r Univariate Analysis of Age;Distance and Unit Area Price,fig.height=8, fig.width=10}
#Univariate Analysis of Age,Distance, Unit Area Price

par(mfrow = c(3,3))
#AGE#

#Histogram
hist(real_estate_df$age,xlab = "Age",ylab = "Number of txn", main = "Histogram of Age")

#Density plot
plot(density(real_estate_df$age),main = "Density plot of Age")

#Boxplot
boxplot(real_estate_df$age,main = toupper("Boxplot of Age"),ylab = "Age (years)")

#DISTANCE#

#Histogram
hist(real_estate_df$distance,xlab = "Distance",ylab = "Number of txn", main = "Histogram of distance")

#Density plot
plot(density(real_estate_df$distance),main = "Density plot of Distance")

#Boxplot
boxplot(real_estate_df$distance,main = toupper("Boxplot of Distance"),ylab = "Distance (meters)")

#Unit Area Price#

#Histogram
hist(real_estate_df$unit_area_price,xlab = "Unit Area Price",ylab = "Number of txn", main = "Histogram of Unit Area Price")

#Density plot
plot(density(real_estate_df$unit_area_price),main = "Density plot of Unit Area Price")

#Boxplot
boxplot(real_estate_df$unit_area_price,main = toupper("Boxplot of Unit Area Price"),ylab = "New Taiwan Dollar/Ping,")
```
**From the density plots and histograms we can observe that Distance,Unit Area price are right skewed**
**From the density plots and histograms we can observe that Age is not normally distributed**
**From the boxplot of Age, we did found any outliers**
**From the boxplot of Unit Area Price, we found that there are 2-3 points which are outliers**
**From the boxplot of Distance, we found there are many points which are outliers**

## Bivariate Analysis
```{r Bivariate Analysis Price vs. Distance; Price vs. Age,fig.height=5, fig.width=10}
#Price vs Age,Distance 
par(mfrow = c(1,2))

#Price vs Distance
plot(unit_area_price ~ distance,data = real_estate_df,xlab = "Distance",ylab = "Unit Area Price",
     main = "Unit Area Price vs. Distance",pch  = 20,cex  = 1,col  = "dodgerblue")
abline(lm(real_estate_df$unit_area_price ~ real_estate_df$distance), col = "darkorange")

#Price vs Age
plot(unit_area_price ~ age,data = real_estate_df,xlab = "Age",ylab = "Unit Area Price",
     main = "Unit Area Price vs. Age",pch  = 20,cex  = 1,col  = "dodgerblue")
abline(lm(real_estate_df$unit_area_price ~ real_estate_df$age), col = "darkorange")
```

**Houses which are closer to nearest MRT station have higher prices and this -ve relationship between price and distance seems strong**

**Houses which are new  have higher prices.However, this -ve relationship between price and distance is week**

```{r Bivariate Analysis: Price vs Transaction Month; Price vs Number of Stores,fig.height=6, fig.width=12}

require(gridExtra)

plot1 = ggplot(real_estate_df, aes(x = txn_month, y = unit_area_price, fill = txn_month)) + geom_boxplot() +
    stat_summary(fun = "mean", geom = "point", shape = 1,size = 1, color = "white") + 
    ggtitle("Price vs. Transaction Month") +
    theme(plot.title = element_text(hjust = 0.5))+
    xlab("Transaction Month") + 
    ylab("Unit Area Price")

plot2 = ggplot(real_estate_df, aes(x = stores, y = unit_area_price,fill = stores)) + geom_boxplot() +
    stat_summary(fun = "mean", geom = "point", shape = 1,size = 1, color = "white") + 
    ggtitle("Price vs. Number Of Stores") +
    theme(plot.title = element_text(hjust = 0.5))+
    xlab("Number Of Stores") + 
    ylab("Unit Area Price")

grid.arrange(plot1, plot2, ncol=2)

```
**Median unit area price does not vary significantly by the month**
**Median unit area price varies significantly by the number of stores. In general, higher the number of stores, higher is the unit area price**
**The Unit Area price shows maximum variation in the month of July 2013 and minimum variation in the month of Jan 2013**

```{r Bivariate Analysis: Price vs Location, fig.height=8, fig.width=12, warning=FALSE}
# Dividing house-prices into 3 categories based on the histogram of the unit are price (which was plotted above)

real_estate_df$price_cat <- "Value"
real_estate_df$price_cat[real_estate_df$unit_area_price < 30] <- "Low"
real_estate_df$price_cat[real_estate_df$unit_area_price >= 30 & real_estate_df$unit_area_price < 60] <- "Mid"
real_estate_df$price_cat[real_estate_df$unit_area_price >= 60] <- "High"
real_estate_df$price_cat = as.factor(real_estate_df$price_cat)

library(ggmap)
qmplot(
  longitude,
  latitude,
  data = real_estate_df,
  zoom = 14,
  colour = price_cat,
  size = I(4),
  darken = .5
) + geom_point() + scale_color_manual(values = c(
  "Low" = "yellow",
   "Mid" = "orange",
  "High" = "red"
))

```
**From the geographical plot, we can see that many houses which are clustered together have high,medium unit area price whereas the houses which are on outskirts have less unit area price. Thus, location of the houses seesm to play an important role in determining the unit area price of the house**

## Correlation Matrix
```{r}
#taking only numerical columns
real_estate_num_df = real_estate_df %>% dplyr::select(unit_area_price,age,distance,latitude,longitude)

#correlations
library(corrplot)

res = cor(real_estate_num_df)

corrplot(res, order = "hclust",addCoef.col = "red", tl.col = "black", tl.srt = 45)

# corrplot(res, type = "upper",order = "hclust", 
#          tl.col = "black", tl.srt = 45, tl.cex = 0.7, number.cex = 0.75,
#          method = 'color', addCoef.col = "red")

```
**Unit area price shows a moderate/high negative correlation with the distance (-0.67). Thus, as distance from the nearest MRT station increases, unit area price decreases**
**Unit area price is shows a low negative correlation with the age (-0.21). Thus, an older house generally will have low unit area price**
**Unit area price shows a moderate positive correlation with both latitude(0.55) and longitude (0.52).Thus, location of the house plays an important role in determining its price**

# Model Building

## Splitting the data into train and test

```{r Split into Test and Train}
require(caTools)
set.seed(1)
sample = sample.split(real_estate_df$unit_area_price, SplitRatio = 0.8)
train = subset(real_estate_df[,-9], sample == TRUE)
test  = subset(real_estate_df[,-9], sample == FALSE)
```

## MLR-1
```{r MLR model}
model = lm(unit_area_price ~ ., data=train)
summary(model)
```

The model's R-squared value is 0.6121.

**We see that for txn_month, all levels except for Feb have coefficients as NA, prediction from a rank-deficient fit may be misleading hence, we should drop the txn_month variable and re-fit the model.**

### Model diagnostics for MLR-1

#### Checking for Linear Regression model assumptions

##### 1. Constant Variance

```{r Constant Variance}
library(lmtest)
plot(model, which=1)
bptest(model)
```

**As the p-value of Breusch-Pagan Test is 0.03577 < 0.05, hence we reject the null hypothesis of homocedasticity.**

**We conclude that there is non-constant variance of errors.We need to apply variance-stabilizing transformation.**

##### 2. Normality Test

```{r}
plot(model, which=2)
shapiro.test(residuals(model))
```
**As the p-value of Shapiro Wilk Test is < 0.05, we reject the null hypothesis and conclude that the normality assumption of errors doesn't hold true.**


##### 3. Auto-correlation of Errors

```{r}
dwtest(model)
```

As the p-value of Durbin-Watson Test is >0.05, we fail to reject the null hypothesis and conclude that the errors are not autocorrelated.

##### 4. Linearity

```{r}
library(MASS)
boxcox(model)
```

**We need to do transformation on the response as suggested by the box-cox plot. The value of lambda that maximizes the log-likelihood is 0.2. Since lambda=1 is not included in the CI, we should apply a transformation to the response. The value of lambda = 0 almost lies in the 95% Confidence Interval. The log transformation can be selected as an optimal transformation.**

##### 5. Checking for numerical predictors transformation

```{r}
par(mfrow = c(1,2))
plot(train$age, model$residuals, main ="Residual vs. Age")
lines(supsmu(train$age, model$residuals), col = "red")
```
```{r}

plot(train$distance, model$residuals, main ="Residual vs. Distance")

lines(supsmu(train$distance, model$residuals), col = "red")
```

**Since in the plot of residuals vs 'distance' we have large number of observations which are concentrated at low values of distance and very few at the higher end of distance. Therefore, it makes sense to do logarithmic transformation of the distance variable for our future analysis**

**Since in the plot of residuals vs 'age', we see the data points being uniformly scattered along the 0 residual value , we do not require any transformation in this case.**

##### 6. Checking for MultiCollinearity
```{r warning=FALSE}
# Collinearity test using variance inflation factor
library(faraway)
colli_check=vif(model)
sort(colli_check)
```

**Since the vif value for none of the variables is >10, we conclude that our predictors are not strongly correlated. Thus, we will not drop any of the above predictors for our future analysis** 

#### Checking Unusual Observations

**As the linear model results are greatly affected by unusual observations, we  test
for high-leverage points, outliers and influential observations.**

##### 1. High leverage points

txn_date(11)+txn_month(11) + age + distance + stores (10)+ latitude + longitude
```{r}
p=36
n=nrow(train)
lev=influence(model)$hat
lev[lev>2*p/n]
```

```{r}
halfnorm(lev,12,labs=row.names(train),ylab="Leverages")
```
**There are no high leverage observations.**

##### 2. Influential Observations
```{r}
cook = cooks.distance(model)
max(cook)
```

```{r}
library(faraway)
halfnorm(cook, labs=row.names(real_estate_df), ylab="Cook's distances")
```

**Observation 221 with a cook's distance value of 0.268 is although <1 but it is much greater as compared to other observations in the data hence, we can consider dropping this observation.**

##### 3. Outliers
```{r}
jack=rstudent(model)
nn=dim(train)[1]
qt(0.5/(2*nn),model$df.residual-1)
```

```{r}
sort(abs(jack),decreasing=TRUE)[1:10]
```

**Observation with r-studentized quantile value >3.2 are considered as outliers.**
**The observations 271,114,221,48,127,149 are outliers.**
**They need to be removed and the model should be re-fitted.**

### Conclusions for MLR-1
Now, we will be performing the following tasks before re-fitting the model:

1. Taking log transformation of the unit area price (response)
2. Taking log transformation of the distance predictor
3. Removing outliers: 271,114,221,48,127,149 
4. Dropping 'txn_month' variable.

### Testing & Training errors errors for MLR-1
```{r}
test_pred1=predict(model,test[,c("txn_date","age" , "distance" , "stores" , "latitude" ,
             "longitude","txn_month")])
train_pred1=predict(model,train[,c("txn_month","age" , "distance" , "stores" , "latitude" ,
             "longitude","txn_date")])
training_error=sum((train$unit_area_price-train_pred1)^2)/nrow(train)
testing_error=sum((test$unit_area_price-test_pred1)^2)/nrow(test)

print(c(training_error,testing_error))
```


## MLR-2

```{r Preparing data fro MLR-2}
train$transformed_price=log(train$unit_area_price)
train$transformed_distance=log(train$distance)
train_new=train[-c(271,114,221,48,127,149),-c(3,7,8)]

test$transformed_price=log(test$unit_area_price)
test$transformed_distance=log(test$distance)
test_new=test[,-c(3,7,8)]
```

```{r}
# MLR-2
model2 = lm(transformed_price ~., data=train_new)
summary(model2)
```

**We see that the r-squared value has increased from 0.6121 to 0.7399. All the predictors are significant at 5% level**

### Model Diagonistics for MLR-2

#### 1. Constant Variance

```{r}
library(lmtest)
plot(model2, which=1)
bptest(model2)
```

**As the p-value of Breusch-Pagan Test is 0.404 which is less than 0.05, hence we can say that the constant variance of errors assumption holds true. **

#### 2.Normality Test

```{r}
plot(model2, which=2)
shapiro.test(residuals(model2))
```

**As the p-value of Shapiro Wilk Test of < 0.05, we can say that the normality assumption of errors does not hold true.However, the graph Q-Q plot looks good. Thus, we can assume that normality assumption is not being violated to a great extent**


#### 3.Auto-correlation of Errors

```{r}
dwtest(model2)
```

**As the p-value of Durbin-Watson Test is >0.05, we fail to reject the null hypothesis and conclude that the errors are not autocorrelated.**

```{r}
test_pred2=predict(model2,test_new[,-6])
train_pred2=predict(model2,train_new[,-6])

#Getting back predictions and actuals in original scale
Predictions_train = exp(train_pred2)
Predictions_test = exp(test_pred2)
Actuals_train = exp(train_new$transformed_price)
Actuals_test = exp(test_new$transformed_price)

#Training and Testing error
training_error2 = sqrt(mean((Predictions_train - Actuals_train)^2))
testing_error2 = sqrt(mean((Predictions_test - Actuals_test)^2))


print(c(training_error2,testing_error2))
```

**We see huge drop in training error and testing error as compared to previous results.Thus, our transfomations and outlier removal is making a better model**

## Ridge Regression

**Letting it handle multi-collinearity by taking both transaction date and transaction month**
```{r Trining the ridge model}
# Preparing train and test data for ridge regression
library(glmnet)
train_ridge1=train_new
train_ridge1$txn_month=train$txn_month[-c(271,114,221,48,127,149)]

train1_y <- train_ridge1$transformed_price
train1_x= model.matrix( ~ .,data= train_ridge1)[,-1]

# Creating a grid for number of folds
folds_grid<-seq(3,15,1)
folds_val=c()
min_lambda=c()
min_cv_error=c()
set.seed(1)
for(fold in folds_grid){
  ridge.fit = cv.glmnet(train1_x[,-25],
                      train1_y,
                      nfolds = fold,
                      alpha = 0)
  folds_val=append(folds_val,fold)
  min_lambda=append(min_lambda,ridge.fit$lambda.min)
  min_cv_error=append(min_cv_error,min(ridge.fit$cvm))
}

# Creating a dataframe for best fold value, its corresponding cv error and 
# optimal lambda
df_ridge1<- data.frame(folds_val, min_lambda, min_cv_error)
best_ridge1<-df_ridge1[which.min(min_cv_error),]

# Fitting the ridge model with optimal parameters
ridge.best1=cv.glmnet(train1_x[, -25],
                      train1_y,
                      nfolds = best_ridge1$folds_val,
                      alpha = 0)
coef(ridge.best1, s="lambda.min")
```
```{r}
par(mfrow = c(1,2))

#Cross validation error vs  lambda
plot(ridge.best1)

#Ridge coeffiients as function of lambda
plot(ridge.best1$glmnet.fit, "lambda")

```

```{r Ridge Regression - RMSE}
# Making train and test predictions
test_ridge1=test_new
test_ridge1$txn_month=test$txn_month

test1_x = model.matrix(~ ., data = test_ridge1)[,-1]

pred_ridge_test1 <- predict(ridge.best1, test1_x[,-25])
pred_ridge_train1 <- predict(ridge.best1, train1_x[,-25])

#Getting back predictions and actuals in original scale
Predictions_train_ridge1 = exp(pred_ridge_train1)
Predictions_test_ridge1 = exp(pred_ridge_test1)
Actuals_train_ridge1 = exp(train_ridge1$transformed_price)
Actuals_test_ridge1 = exp(test_ridge1$transformed_price)

#Training and Testing error
rmse_train_ridge1 = sqrt(mean((Predictions_train_ridge1 - Actuals_train_ridge1)^2))
rmse_test_ridge1 = sqrt(mean((Predictions_test_ridge1 - Actuals_test_ridge1)^2))

print(c(rmse_train_ridge1, rmse_test_ridge1))
```
## Regression Trees
```{r Fitting Initial Regression Tree}
library(rpart)
set.seed(1)

train_tree1=train[,1:8]
test_tree1=test[,1:8]

# Fitting a 5-fold cross-validation CART model library(rpart)
rpart.fit = rpart(
  unit_area_price ~.,
  data = train_tree1,
  control = rpart.control(xval = 5)
)
# plot tree
plot(rpart.fit, uniform=TRUE,
   main="Unpruned Regression Tree for unit_area_price")
text(rpart.fit, use.n=TRUE, all=TRUE, cex=.8)
```

```{r Tuning 1 : Regression Tree}
rpart.fit$cptable
```

```{r Tuning 2: Regression Tree}
# Index corresponding to min xerror
library(rpart.plot)

min_ind = which.min(rpart.fit$cptable[, 4]) # CP corresponding to min xerror
cp_min = rpart.fit$cptable[min_ind, 1]
# Tree with smallest cross-validation error
prunedtree = prune(rpart.fit, cp = cp_min)
rpart.plot(prunedtree)
#summary(prunedtree) # detailed summary of splits
```

```{r}
par(mfrow=c(1,3))
rsq.rpart(prunedtree) # visualize cross-validation results
plotcp(prunedtree)  # visualize cross-validation results
```

```{r Regression Tree - RMSE}
## Train RMSE
tree_predicts_train=prunedtree %>% predict(train_tree1[,-7])
tree_rmse_train=(sum((train_tree1[,7]-tree_predicts_train)^2)/nrow(train_tree1))^0.5

# Test RMSE
tree_predicts=prunedtree %>% predict(test_tree1[,-7])
tree_rmse=(sum((test_tree1[,7]-tree_predicts)^2)/nrow(test_tree1))^0.5

print(c(tree_rmse_train,tree_rmse))
```
## Random Forest
```{r Initial RF (Untransformed Variables)}
set.seed(1)

#install.packages('randomForest')
library(randomForest)

#removing the transformed price and distance that were created during MLR
train_rf = train[,-c(9,10)]
test_rf = test[,-c(9,10)]

#Fitting the random forest model without tuning the hyperparameters
rf.fit1 = randomForest(unit_area_price ~., data = train_rf, ntree = 50)

#Summary from the random forest model
rf.fit1

#Plot to check the ntree value when oob error stabilize
plot(rf.fit1)

#Prediction on test data
rf.fit1_pred = predict(rf.fit1, test_rf)

#Training error in terms of response units
rf.fit1_trainerror = sqrt(mean((rf.fit1$predicted - train_rf$unit_area_price)^2))
sprintf("Training Error from the untuned random forest: %f (10000 New Taiwan Dollar/Ping)",round(rf.fit1_trainerror,2))

#Testing error in terms of response units
rf.fit1_testerror = sqrt(mean((rf.fit1_pred - test_rf$unit_area_price)^2))
sprintf("Testing Error from the untuned random forest: %f (10000 New Taiwan Dollar/Ping)",round(rf.fit1_testerror,2))

```
```{r RF: Variable Importance}

varImpPlot(rf.fit1)
```
